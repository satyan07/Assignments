{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading Stem: Package 'Stem' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('Stem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())\n",
    "print(len(brown.categories()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Dan', 'Morgan', 'told', 'himself', 'he', 'would', 'forget', 'Ann', 'Turner', '.'], ['He', 'was', 'well', 'rid', 'of', 'her', '.'], ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = brown.sents(categories = 'adventure')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scotty did not go back to school .'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = brown.sents(categories = 'fiction')\n",
    "' '.join(data[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = '''It was a very pleasant day.The weather was cool and there were light showers.\n",
    "I went to the market to buy some fruits'''\n",
    "\n",
    "sentence = \"Send all the 50 documents related to chapter 1,2,3 at prateek@cb.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It was a very pleasant day.The weather was cool and there were light showers.', 'I went to the market to buy some fruits']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "sents = sent_tokenize(document)\n",
    "print(sents)\n",
    "print(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send all the 50 documents related to chapter 1,2,3 at prateek@cb.com']\n"
     ]
    }
   ],
   "source": [
    "sents = sent_tokenize(sentence)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', '50', 'documents', 'related', 'to', 'chapter', '1,2,3', 'at', 'prateek', '@', 'cb.com']\n",
      "['Send', 'all', 'the', '50', 'documents', 'related', 'to', 'chapter', '1,2,3', 'at', 'prateek@cb.com']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(sentence)\n",
    "print(words)\n",
    "split = sentence.split()\n",
    "print(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'then', 'd', 'haven', 'having', 'y', 'been', 'just', \"aren't\", 'wouldn', 'or', 'yourself', \"haven't\", \"you'll\", 'hasn', 'the', 'at', 'what', 'as', 'my', 'when', 'was', 'doesn', 'more', 'aren', 'him', 'am', 'ma', 'now', 'above', 'no', 'can', 'theirs', 't', 'didn', 'own', 'why', 'ain', 'mightn', 'itself', 'shouldn', 'this', 'than', 's', 'about', 're', 'has', 'such', 'once', 'because', 'we', 'each', 'into', 'a', 'm', 'with', \"shan't\", 'is', 'weren', \"mightn't\", 'ourselves', 'she', \"you're\", 'further', 'yours', 'do', 'while', 'for', 'should', 'needn', 'our', 'after', \"you've\", 'herself', 'through', \"you'd\", 'in', 'you', 'to', \"don't\", 'down', 'those', 'out', 'hadn', 'doing', 'will', 'myself', 'an', 'against', 'but', \"it's\", 'll', 'only', \"wasn't\", 'very', 'on', \"didn't\", 'couldn', 'who', 'by', \"needn't\", 'did', 'he', 'had', \"couldn't\", 'hers', 'off', 'your', 'up', 'wasn', 'both', 'it', 'shan', 'their', \"doesn't\", 'most', \"she's\", 'her', \"weren't\", 'and', 'same', 'during', 'here', 'too', \"that'll\", 'below', \"shouldn't\", 'from', 'there', 'under', 'themselves', 'have', 'that', 'all', 'be', 'how', 'until', 'they', \"wouldn't\", 'yourselves', \"hasn't\", 'between', 'were', 'some', 'won', 'mustn', 'so', 've', 'if', 'of', 'does', \"should've\", 'being', 'which', 'o', 'over', 'not', 'any', 'himself', \"mustn't\", 'its', 'other', 'whom', 'ours', 'nor', \"hadn't\", 'them', 'before', 'don', 'are', \"isn't\", 'i', 'again', \"won't\", 'isn', 'where', 'me', 'his', 'few', 'these'}\n"
     ]
    }
   ],
   "source": [
    "sw = set (stopwords.words('english'))\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'bothered', 'much']\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwards(text,sw):\n",
    "    useful_words =[w for w in text if w not in sw]\n",
    "    return useful_words\n",
    "    \n",
    "text = \"I am not bothered about her very much\".split()\n",
    "useful_words = remove_stopwards(text , sw)\n",
    "print(useful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', 'documents', 'related', 'to', 'chapter', 'at', 'prateek@cb', 'com']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('[A-Za-z@]+')\n",
    "useful_text = tokenizer.tokenize(sentence)\n",
    "print(useful_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Foxes love to make jump.The quick brown fox was seen jumping over the lovely dog\n",
    "          from a 6ft feet high wall\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "ps.stem('jumping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('jumps')\n",
    "ps.stem('lovely')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Snowball Stemmer'''\n",
    "ss = SnowballStemmer('english')\n",
    "ss.stem('lovely')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem('jumped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''Lemmatization'''\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# wn = WordNetLemmatizer()\n",
    "# wn.lemmatize('jumping')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Indian cricket team will wins World Cup,says Capt. Virat Kohli.World cup will be held at Sri Lanka',\n",
    "    'We will win next Lok Sabha Elections,says confident Indian PM',\n",
    "    'The nobel laurate won the hearts of the people',\n",
    "    'The movie Raazi is an exciting Indian Spy thriller based on a real story'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 1 0 1 2 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0\n",
      " 2 0 1 0 2]\n",
      "42\n",
      "[array(['at', 'be', 'capt', 'cricket', 'cup', 'held', 'indian', 'kohli',\n",
      "       'lanka', 'says', 'sri', 'team', 'virat', 'will', 'wins', 'world'],\n",
      "      dtype='<U9')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "vectorized_corpus = cv.fit_transform(corpus)\n",
    "vectorized_corpus = vectorized_corpus.toarray()\n",
    "print(vectorized_corpus[0])\n",
    "print(len(vectorized_corpus[0]))\n",
    "\n",
    "numbers = vectorized_corpus[0]\n",
    "s = cv.inverse_transform(numbers)\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 2 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 2]\n",
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array(['capt', 'cricket', 'cup', 'held', 'indian', 'kohli', 'lanka',\n",
       "        'says', 'sri', 'team', 'virat', 'wins', 'world'], dtype='<U9'),\n",
       " array(['confident', 'elections', 'indian', 'lok', 'next', 'pm', 'sabha',\n",
       "        'says', 'win'], dtype='<U9'),\n",
       " array(['hearts', 'laurate', 'nobel', 'people'], dtype='<U9'),\n",
       " array(['based', 'exciting', 'indian', 'movie', 'raazi', 'real', 'spy',\n",
       "        'story', 'thriller'], dtype='<U9')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cv.vocabulary_\n",
    "\n",
    "def myTokenizer(document):\n",
    "    words = tokenizer.tokenize(document.lower())\n",
    "    words = remove_stopwards(words,sw)\n",
    "    return words\n",
    "\n",
    "\n",
    "cv = CountVectorizer(tokenizer = myTokenizer)\n",
    "vectorized_corpus = cv.fit_transform(corpus)\n",
    "vectorized_corpus = vectorized_corpus.toarray()\n",
    "print(vectorized_corpus[0])\n",
    "print(len(vectorized_corpus[0]))\n",
    "\n",
    "cv.inverse_transform(vectorized_corpus)\n",
    "'''fi_transform for the training data and the transform for the test data'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 2 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0\n",
      " 0 0 2 1 1 0 0 1 1 0 0 0 1 1 1 0 0 0 2 2 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'indian': 38,\n",
       " 'cricket': 18,\n",
       " 'team': 93,\n",
       " 'will': 113,\n",
       " 'wins': 123,\n",
       " 'world': 129,\n",
       " 'cup': 21,\n",
       " 'says': 82,\n",
       " 'capt': 12,\n",
       " 'virat': 107,\n",
       " 'kohli': 47,\n",
       " 'be': 9,\n",
       " 'held': 35,\n",
       " 'at': 3,\n",
       " 'sri': 90,\n",
       " 'lanka': 50,\n",
       " 'indian cricket': 39,\n",
       " 'cricket team': 19,\n",
       " 'team will': 94,\n",
       " 'will wins': 118,\n",
       " 'wins world': 124,\n",
       " 'world cup': 130,\n",
       " 'cup says': 22,\n",
       " 'says capt': 83,\n",
       " 'capt virat': 13,\n",
       " 'virat kohli': 108,\n",
       " 'kohli world': 48,\n",
       " 'cup will': 24,\n",
       " 'will be': 114,\n",
       " 'be held': 10,\n",
       " 'held at': 36,\n",
       " 'at sri': 4,\n",
       " 'sri lanka': 91,\n",
       " 'indian cricket team': 40,\n",
       " 'cricket team will': 20,\n",
       " 'team will wins': 95,\n",
       " 'will wins world': 119,\n",
       " 'wins world cup': 125,\n",
       " 'world cup says': 131,\n",
       " 'cup says capt': 23,\n",
       " 'says capt virat': 84,\n",
       " 'capt virat kohli': 14,\n",
       " 'virat kohli world': 109,\n",
       " 'kohli world cup': 49,\n",
       " 'world cup will': 132,\n",
       " 'cup will be': 25,\n",
       " 'will be held': 115,\n",
       " 'be held at': 11,\n",
       " 'held at sri': 37,\n",
       " 'at sri lanka': 5,\n",
       " 'we': 110,\n",
       " 'win': 120,\n",
       " 'next': 60,\n",
       " 'lok': 54,\n",
       " 'sabha': 79,\n",
       " 'elections': 26,\n",
       " 'confident': 15,\n",
       " 'pm': 73,\n",
       " 'we will': 111,\n",
       " 'will win': 116,\n",
       " 'win next': 121,\n",
       " 'next lok': 61,\n",
       " 'lok sabha': 55,\n",
       " 'sabha elections': 80,\n",
       " 'elections says': 27,\n",
       " 'says confident': 85,\n",
       " 'confident indian': 16,\n",
       " 'indian pm': 41,\n",
       " 'we will win': 112,\n",
       " 'will win next': 117,\n",
       " 'win next lok': 122,\n",
       " 'next lok sabha': 62,\n",
       " 'lok sabha elections': 56,\n",
       " 'sabha elections says': 81,\n",
       " 'elections says confident': 28,\n",
       " 'says confident indian': 86,\n",
       " 'confident indian pm': 17,\n",
       " 'the': 96,\n",
       " 'nobel': 63,\n",
       " 'laurate': 51,\n",
       " 'won': 126,\n",
       " 'hearts': 32,\n",
       " 'of': 66,\n",
       " 'people': 72,\n",
       " 'the nobel': 101,\n",
       " 'nobel laurate': 64,\n",
       " 'laurate won': 52,\n",
       " 'won the': 127,\n",
       " 'the hearts': 97,\n",
       " 'hearts of': 33,\n",
       " 'of the': 67,\n",
       " 'the people': 103,\n",
       " 'the nobel laurate': 102,\n",
       " 'nobel laurate won': 65,\n",
       " 'laurate won the': 53,\n",
       " 'won the hearts': 128,\n",
       " 'the hearts of': 98,\n",
       " 'hearts of the': 34,\n",
       " 'of the people': 68,\n",
       " 'movie': 57,\n",
       " 'raazi': 74,\n",
       " 'is': 44,\n",
       " 'an': 0,\n",
       " 'exciting': 29,\n",
       " 'spy': 87,\n",
       " 'thriller': 104,\n",
       " 'based': 6,\n",
       " 'on': 69,\n",
       " 'real': 77,\n",
       " 'story': 92,\n",
       " 'the movie': 99,\n",
       " 'movie raazi': 58,\n",
       " 'raazi is': 75,\n",
       " 'is an': 45,\n",
       " 'an exciting': 1,\n",
       " 'exciting indian': 30,\n",
       " 'indian spy': 42,\n",
       " 'spy thriller': 88,\n",
       " 'thriller based': 105,\n",
       " 'based on': 7,\n",
       " 'on real': 70,\n",
       " 'real story': 78,\n",
       " 'the movie raazi': 100,\n",
       " 'movie raazi is': 59,\n",
       " 'raazi is an': 76,\n",
       " 'is an exciting': 46,\n",
       " 'an exciting indian': 2,\n",
       " 'exciting indian spy': 31,\n",
       " 'indian spy thriller': 43,\n",
       " 'spy thriller based': 89,\n",
       " 'thriller based on': 106,\n",
       " 'based on real': 8,\n",
       " 'on real story': 71}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Unigram - single feature, single word'''\n",
    "# cv =  CountVectorizer(ngram_range = (2,2))\n",
    "cv = CountVectorizer(ngram_range=(1,3))\n",
    "vectorized_corpus = cv.fit_transform(corpus)\n",
    "vectorized_corpus = vectorized_corpus.toarray()\n",
    "print(vectorized_corpus[0])\n",
    "cv.vocabulary_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.21074652 0.         0.21074652 0.21074652 0.\n",
      " 0.21074652 0.42149305 0.         0.         0.         0.21074652\n",
      " 0.13451678 0.         0.21074652 0.21074652 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.16615498 0.\n",
      " 0.21074652 0.         0.21074652 0.         0.         0.21074652\n",
      " 0.         0.33230996 0.         0.21074652 0.         0.42149305]\n",
      "42\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.29368184 0.\n",
      " 0.         0.         0.         0.         0.29368184 0.\n",
      " 0.         0.         0.29368184 0.29368184 0.         0.29368184\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.69462641 0.         0.\n",
      " 0.         0.         0.         0.         0.29368184 0.        ]\n",
      "42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'indian': 12,\n",
       " 'cricket': 6,\n",
       " 'team': 32,\n",
       " 'will': 37,\n",
       " 'wins': 39,\n",
       " 'world': 41,\n",
       " 'cup': 7,\n",
       " 'says': 28,\n",
       " 'capt': 4,\n",
       " 'virat': 35,\n",
       " 'kohli': 14,\n",
       " 'be': 3,\n",
       " 'held': 11,\n",
       " 'at': 1,\n",
       " 'sri': 30,\n",
       " 'lanka': 15,\n",
       " 'we': 36,\n",
       " 'win': 38,\n",
       " 'next': 19,\n",
       " 'lok': 17,\n",
       " 'sabha': 27,\n",
       " 'elections': 8,\n",
       " 'confident': 5,\n",
       " 'pm': 24,\n",
       " 'the': 33,\n",
       " 'nobel': 20,\n",
       " 'laurate': 16,\n",
       " 'won': 40,\n",
       " 'hearts': 10,\n",
       " 'of': 21,\n",
       " 'people': 23,\n",
       " 'movie': 18,\n",
       " 'raazi': 25,\n",
       " 'is': 13,\n",
       " 'an': 0,\n",
       " 'exciting': 9,\n",
       " 'spy': 29,\n",
       " 'thriller': 34,\n",
       " 'based': 2,\n",
       " 'on': 22,\n",
       " 'real': 26,\n",
       " 'story': 31}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "vc = tfidf.fit_transform(corpus).toarray()\n",
    "print(vc[0])\n",
    "print(len (vc[0]))\n",
    "\n",
    "print(vc[2])\n",
    "print(len(vc[2]))\n",
    "\n",
    "tfidf.vocabulary_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
